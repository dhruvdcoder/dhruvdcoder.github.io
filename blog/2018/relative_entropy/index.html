<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Relative Entropy and its role as a cost function for machine learning tasks | Dhruvesh Patel</title> <meta name="author" content="Dhruvesh Patel"> <meta name="description" content="Explains Relative Entropy from different perspectives and how it is used to derive cost functions for various ML tasks like classification, etc,."> <meta name="keywords" content="Machine Learning, mathematics"> <meta property="og:site_name" content="Dhruvesh Patel"> <meta property="og:type" content="article"> <meta property="og:title" content="Dhruvesh Patel | Relative Entropy and its role as a cost function for machine learning tasks"> <meta property="og:url" content="https://dhruveshp.com/blog/2018/relative_entropy/"> <meta property="og:description" content="Explains Relative Entropy from different perspectives and how it is used to derive cost functions for various ML tasks like classification, etc,."> <meta property="og:image" content="/relative_entropy/binary_channel.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Relative Entropy and its role as a cost function for machine learning tasks"> <meta name="twitter:description" content="Explains Relative Entropy from different perspectives and how it is used to derive cost functions for various ML tasks like classification, etc,."> <meta name="twitter:image" content="/relative_entropy/binary_channel.jpg"> <meta name="twitter:site" content="@_dhruveshp"> <meta name="twitter:creator" content="@_dhruveshp"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Dhruvesh  Patel"
        },
        "url": "https://dhruveshp.com/blog/2018/relative_entropy/",
        "@type": "BlogPosting",
        "description": "Explains Relative Entropy from different perspectives and how it is used to derive cost functions for various ML tasks like classification, etc,.",
        "headline": "Relative Entropy and its role as a cost function for machine learning tasks",
        "sameAs": ["https://scholar.google.com/citations?user=6F2CvwoAAAAJ", "https://github.com/dhruvdcoder", "https://twitter.com/_dhruveshp"],
        "name": "Dhruvesh  Patel",
        "@context": "https://schema.org"
    }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dhruveshp.com/blog/2018/relative_entropy/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?b67d38c35ebb9c65488d84d1cd53bc55"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Dhruvesh </span>Patel</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/_pages/notes/">notes</a> </li> <li class="nav-item "> <a class="nav-link" href="/timeline/">timeline</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Relative Entropy and its role as a cost function for machine learning tasks</h1> <p class="post-meta">April 15, 2018</p> <p class="post-tags"> <a href="/blog/2018"> <i class="fas fa-calendar fa-sm"></i> 2018 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fas fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/tag/mathematics"> <i class="fas fa-hashtag fa-sm"></i> mathematics</a>     ·   <a href="/blog/category/machinelearning"> <i class="fas fa-tag fa-sm"></i> Machinelearning</a>   <a href="/blog/category/mathematics"> <i class="fas fa-tag fa-sm"></i> mathematics</a>   </p> </header> <article class="post-content"> <div> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog//relative_entropy/binary_channel.jpg"> </div> <div id="markdown-content"> <h1 class="no_toc" id="contents">Contents</h1> <ul id="markdown-toc"> <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li> <li> <a href="#relative-entropy" id="markdown-toc-relative-entropy">Relative Entropy</a> <ul> <li><a href="#statistical-perspective" id="markdown-toc-statistical-perspective">Statistical perspective</a></li> <li><a href="#information-theoretic-perspective" id="markdown-toc-information-theoretic-perspective">Information theoretic perspective</a></li> </ul> </li> <li> <a href="#quantities-derived-from-relative-entropy" id="markdown-toc-quantities-derived-from-relative-entropy">Quantities derived from Relative Entropy</a> <ul> <li><a href="#shannons-entropy" id="markdown-toc-shannons-entropy">Shannon’s Entropy</a></li> <li><a href="#cross-entropy" id="markdown-toc-cross-entropy">Cross Entropy</a></li> <li><a href="#conditional-entropy" id="markdown-toc-conditional-entropy">Conditional Entropy</a></li> <li><a href="#conditional-relative-entropy" id="markdown-toc-conditional-relative-entropy">Conditional Relative Entropy</a></li> <li><a href="#mutual-information" id="markdown-toc-mutual-information">Mutual Information</a></li> </ul> </li> <li> <a href="#relative-entropy-in-machine-learning" id="markdown-toc-relative-entropy-in-machine-learning">Relative Entropy in Machine Learning</a> <ul> <li><a href="#multiclass-classification" id="markdown-toc-multiclass-classification">Multiclass classification</a></li> </ul> </li> <li><a href="#references" id="markdown-toc-references">References</a></li> </ul> <h1 id="introduction">Introduction</h1> <p>I had been seeing terms like entropy, cross-entropy, KL-Divergence, information gain, etc., regularly in association with cost functions in machine learning tasks. For example, <a href="http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy" rel="external nofollow noopener" target="_blank">cross-entropy loss</a> is used as the cost function in multi-class classification problems; <a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy#Prior_probabilities" rel="external nofollow noopener" target="_blank">maximum entropy principle</a> in Bayesian inference, etc. All these quantities seemed related and I decided to find the meaning and the origin of each of these terms. It turns out that all these quantities can be derived from the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="external nofollow noopener" target="_blank">Relative Entropy</a> which is synonymous to KL-Divergence.</p> <p>This post, starts with the description of Relative Entropy and what it means when viewed from various perspectives (statistical, information theoretic, etc.). Then it goes on to derive some other related quantities like Entropy (Shannon’s Entropy), Cross Entropy, Conditional Relative Entropy, etc. Then the last section talks why and how we use Cross Entropy as a loss function in classification.</p> <h1 id="relative-entropy">Relative Entropy</h1> <p>Firstly, it needs to be noted that Relative Entropy has various names which stem from its use in various fields of study. Following are all synonymous:</p> <ul> <li> <p>Relative Entropy</p> </li> <li> <p>KL-Divergence</p> </li> <li> <p>Information Gain</p> </li> <li> <p>KL-Distance (its not a true metric)</p> </li> <li> <p>Discrimination</p> </li> </ul> <p>Relative entropy of a probability distribution \(p(x)\) with respect to \(q(x)\), where \(p, q\) are defined on the same set \(X\) is defined as follows,</p> <p>$$ D_{\mathrm {KL} }(P||Q)=\int _{X}\log {\frac {dP}{dQ}}\,dP $$</p> <p>which for the case of continuous probability distributions over \(X\) becomes,</p> <p>$$ D_{\mathrm {KL} }(p||q)=\int_{X} p(x)\log{\frac{p(x)}{q(x)}}\,dx $$</p> <p>and for discrete distributions, the integration changes to sum,</p> <p>$$ D_{\mathrm {KL} }(p||q)=\sum_{X} p(x)\log{\frac{p(x)}{q(x)}} $$</p> <p>Another point to note here is that this quantity is always greater than or equal to zero. It is zero when \(p=q\). This result is called the <a href="https://en.wikipedia.org/wiki/Gibbs%27_inequality" rel="external nofollow noopener" target="_blank">Gibbs inequality</a>. Now, let us understand the meaning of the quantity from different perspectives.</p> <h2 id="statistical-perspective">Statistical perspective</h2> <p>In the space (<a href="https://en.wikipedia.org/wiki/Statistical_manifold" rel="external nofollow noopener" target="_blank">statistical manifold</a>) of probability distributions (where each distributions is a point) defined over set of events \(X\), relative entropy is <strong>like</strong> <em>distance</em> between two distributions. It is not a true distance metric because it does not satisfy the requirements of a metric (like symmetry). Nevertheless, it is an asymmetric measure of how much a probability distribution diverges from another. Derivation of KL-divergence is beyond the scope of this post, however interested reader is encouraged to check Kullback’s book<sup>[<a href="#1">1</a>]</sup>. If one sees closely, \( D_{\mathrm {KL} }(p||q)\), is the expected value (expectation w.r.t \(p\)) of the random variable \(y=\log{\frac{p(x)}{q(x)}}\) which is a function of the random variable \(X\). \(y\) is nothing but the logarithmic difference of the probablilities of \(X=x\) given by two probability distributions, ie. \(p\) and \(q\). Hence, roughly speaking, Relative Entropy is the expected value (expectation w.r.t \(p\)) of the difference in the probalilities w.r.t \(p\) and \(q\) respectively, for random variable \(X\).</p> <h2 id="information-theoretic-perspective">Information theoretic perspective</h2> <p>The general term entropy can be thought of as the degree of uncertainty about the value of a random variable: More the uncertainity about the value of a random variable, lesser informative is its probability distribution. The informativeness of a probalility density function is can be thought of as the amount of uncertainty (entropy) it can reduce by providing some knowldge about the uncertain event (value of the random variable). For example, if \(X\) is a discrete random variable which can take values from \(\{1, 2, 3\}\) with probabilities \(\{0, 1, 0\}\) then there is no uncertainty (entropy) in the value of \(X\). Instead, if the probablity distribution were to be \(\{1/3, 1/3, 1/3\}\) then all the three values are equally likely and the uncertainty about the value of \(X\) is maximum.</p> <p>Hence, Relative Entropy can be thought of as the change (increase or decrease) in the uncertainty (information) about a random variable when moving to using a new probablity distribution \(p\) instead of old distribution \(q\). For instance, say we have a coin toss experiment and we assume that it is a fair coin, then the probablity distribution is given by \(q(H)=0.5, q(T)=0.5\). Now, someone comes and tells us that the coin was made defective and is biased towards heads with a probablility of \(0.8\) then our new probablity distribution would be \(p(H)=0.8, p(T)=0.2\) and the relative entropy of \(p\) w.r.t \(q\) will be \(0.8\log _2{1.6}+0.2\log _2{0.4} =0.27\). As the following plot shows, the relative entropy for new distribution w.r.t old (uniform) distribution reaches its maximum when there is no uncretainty.</p> <p><img src="https://dhruveshp.com/assets/img/blog/relative_entropy/RE1.png" alt="Plot of relative entropy for the example mentioned above" title="Plot of relative entropy for the example mentioned above"></p> <p>Another good perspective to relative entropy can be from the coding theory. Assume that we have a bag full of balls of colors red, blue, green and orange. We are supposed to draw one ball with replacement and send the result (color of the ball) to our friend sitting faraway in the form of a message using a binary channel. First we need to choose an encoding for the colors and while doing so we wish to minimize the number of bits required per message on average. Let the probability distribution \(q\), for the colors showing up be,</p> <p>$$ q(x) = \begin{cases} 0.4 &amp; \text{,if $ x=$ red} \\\<br> 0.4 &amp; \text{,if $x=$ blue} \\\<br> 0.1 &amp; \text{,if $ x=$ green} \\\<br> 0.1 &amp; \text{,if $ x=$ orange} \end{cases} $$</p> <p>Given this probability distribution, an optimal encoding would have different lengths for different colors with the length of the message for red being shorter than that of orange and so on.</p> <p>Now supose we add a few more balls into the bag and the probablility distribution now changes to \(p\) as follows, $$ p(x) = \begin{cases} 0.1 &amp; \text{,if $ x=$ red} \\\<br> 0.05 &amp; \text{,if $x=$ blue} \\\<br> 0.8 &amp; \text{,if $ x=$ green} \\\<br> 0.05 &amp; \text{,if $ x=$ orange} \end{cases} $$</p> <p>If we still use the same encoding which was designed to be optimal for \(q\) then the expected length of a message would increase. Here, the relative entropy of \(p\) w.r.t \(q\), i.e. \(D_{\mathrm{KL} }(p||q)\) is the <strong>expected change in the length of a message</strong> due to the change in probability distribution.</p> <h1 id="quantities-derived-from-relative-entropy">Quantities derived from Relative Entropy</h1> <h2 id="shannons-entropy">Shannon’s Entropy</h2> <p>Suppose we have a uniform probability distribution:</p> <p>$$ q(x) = \begin{cases} 1/4 &amp; \text{,if $ x=$ red} \\\<br> 1/4 &amp; \text{,if $x=$ blue} \\\<br> 1/4 &amp; \text{,if $ x=$ green} \\\<br> 1/4 &amp; \text{,if $ x=$ orange} \end{cases} $$</p> <p>If we have another distribution \( p \) (which is different from \(q\)) for the same random variable \(X\), then Relative Entropy of \( p \) w.r.t \( q \) would be:</p> <p>$$ \begin{eqnarray} D_{\mathrm{KL} }(p||q)&amp;=&amp;\sum_{X} p(x)\log{\frac{p(x)}{q(x)}} \\<br> &amp;=&amp; \sum_{X} p(x)\log{\frac{p(x)}{1/4}} \\<br> &amp;=&amp;\sum_{X} \left( p(x)\log{p(x)}-\log{1/4} \right) \\<br> &amp;=&amp; \mathrm{constant}-\sum_{X} p(x)\log{\frac{1}{p(x)}} \\<br> &amp;\geq&amp; 0 ~~\text{(using Gibbs’ inequality for relative entropy)} \end{eqnarray} $$</p> <p>Hence, we can say that according to KL-Distance (relative entropy), the distribution \( p \) departs from uniform distribution by \( \sum_{X} p(x)\log{\frac{1}{p(x)}} \) amount. This quantity is denoted as \(H(p)\) and is called Shannon Entropy of probability distribution \( p \). Also, as a consequence of Gibbs’ inequality, it can be seen that the Shannon Entropy of Uniform Distribution is highest amongst all the possible distributions for a random variable.</p> <p>Shannon’s Entropy – sometimes referred to as Entropy of a probability distribution – can be seen from another perspective. It gives the <strong>minimum expected</strong> bits required to encode (on a binary channel) an event taken from an event space with probability distribution \(p\). Here, we say “minimum” because the encoding is optimized for \(p\), i.e., events with higher probability have less number of bits in their encoding and vise versa.</p> <p>$$ H(p) = E_p\left[\log{\frac{1}{p(X)}} \right] $$</p> <h2 id="cross-entropy">Cross Entropy</h2> <p>From information theory perspective, we have seen that</p> <ol> <li> <p>Relative Entropy \( D_{\mathrm{KL} }(p||q) \) is the <strong>difference/change in the expected length</strong> of a message when we change the probablity distribution from \( q \) to \( p \) with encoding optimal for \( q \)</p> </li> <li> <p>Entropy \( H(p) \) is <strong>the expected length</strong> of message when the probablity distribution is \( p \) and the <strong>encoding is optimal for \( p \)</strong></p> </li> </ol> <p>Then what would be <strong>the expected length</strong> of a message when the probablity distribution is \( p \) but <strong>the encoding is optimal for \( q \)</strong>?</p> <p>$$ \begin{eqnarray} C(p,q) &amp;=&amp; E_p\left[\log{\frac{1}{q}} \right] \\<br> &amp;=&amp; \sum_X p(x) \log{\frac{1}{q(x)}} \\<br> &amp;=&amp; \sum_X p(x)\left( \log{\frac{p(x)}{q(x)}} - \log{p(x)} \right) \\<br> &amp;=&amp; D_{\mathrm{KL} }(p||q) + H(p) \end{eqnarray} $$</p> <p><strong>Cross Entropy of \( p \) w.r.t \( q \) is denoted by \( C(p,q) \) and is defined as the expected length of a message when the probability distribution is \( p \) but the encoding is optimal for \( q \)</strong>.</p> <h2 id="conditional-entropy">Conditional Entropy</h2> <p>From here on, let \( X \) be a random variable on event space \( A \) with probablity distribution \( p \) and \( Y \) be a random variable on event space \( B \) with probablity distribution \( q \). Also, lets denote \( H(p,q) \) as \( H(X,Y) \). Then</p> <p>$$ \begin{eqnarray} H(X, Y) &amp;=&amp; \sum_{x,y \in A} P_{XY}(x, y)\log{\left( \frac{1}{P_{XY}(x,y)} \right)} \\<br> &amp;=&amp; \sum_{x,y \in A} \frac{P_{XY}(x, y)}{P_X(x)} P_X(x) \log{\left( \frac{P_X(x)}{P_X(x)P_{XY}(x,y)} \right)} \\<br> &amp;=&amp; \sum_{x,y \in A} P_{Y|X}(x,y) P_X(x) \log{\left( \frac{1}{P_X(x)P_{Y|X}(x,y)} \right)} \\<br> &amp;=&amp; \sum_{x \in A} P_X(x) \sum_{y \in A} P_{Y|X}(x,y) \left( \log{\left( \frac{1}{P_{Y|X}(x,y)} \right)} + \log{\left( \frac{1}{P_X(x)} \right)} \right) \\<br> &amp;=&amp; \sum_{x \in A} P_X(x) \log{\left( \frac{1}{P_X(x)} \right)} \sum_{y \in A} P_{Y|X}(x,y) + \sum_{x \in A} P_X(x) \sum_{y \in A} P_{Y|X}(x,y) \log{\left( \frac{1}{P_{Y|X}(x,y)} \right)} \\<br> &amp;=&amp; H(X) + E_X \left[ H(Y|X=x)\right] \end{eqnarray} $$</p> <p><strong>Here, \( E_X \left[ H(Y|X=x)\right] \) denoted as \( H(Y|X)\) is the Conditional Entropy of \( Y \) given \( X \). It is the expected (expectation w.r.t \( X \)) entropy left in \( Y \) given \( X \).</strong></p> <h2 id="conditional-relative-entropy">Conditional Relative Entropy</h2> <p>As mentioned earlier, Relative Entropy is the expected value of the logarithmic difference in the probabilities with respect to two different probability distributions for a random variable. Just like expectation of any function of random variables, we can condition this expectation on another random variable and take outer expectation w.r.t to that variable. For instance, suppose we have three random variables, \( X, Y, Z \) where \(Y\) and \(Z\) are defined on same event space \(B\) and \(X\) is defined on \( A \). We also have probability distributions \( P_{Y|X}, P_{Z|X}, P_X \). Then the conditional relative entropy between \( P_{Y|X} \) and \( P_{Z|X} \) is:</p> <p>$$ \begin{eqnarray} D_{\mathrm{KL} }\left( P_{Y|X} || P_{Z|X} | P_X \right) &amp;=&amp; E_X \left[ D_{\mathrm{KL} } \left( P_{Y|X} || P_{Z|X} \right) \right] \\<br> &amp;=&amp; \sum_{a \in A} P_X(a) \sum_{b \in B} P_{Y|X}(b) \log{\frac{P_{Y|X}(b)}{P_{Z|X}(b)}} \\<br> &amp;=&amp; \sum_{a \in A} \sum_{b \in B} P_X(a) P_{Y|X}(b) \log{\frac{P_{Y|X}(b) P_X{a}}{P_{Z|X}(b) P_X{a}}} \\<br> &amp;=&amp; \sum_{a \in A} \sum_{b \in B} P_{XY}(a, b) \log{\frac{P_{XY}(a, b)}{P_{XZ}(a, b)}} \\<br> &amp;=&amp; D_{\mathrm{KL} }\left( P_{XY} || P_{XZ} \right) \end{eqnarray} $$</p> <h2 id="mutual-information">Mutual Information</h2> <p>Just like correlation coefficient is a measure of linear relationship between two random variables, Mutual Information is the most general measure of relationship between two random variables. It is the KL-Distance between the joint distribution and the product of marginals.</p> <p>$$ \begin{eqnarray} \mathrm{I}(X,Y) &amp;=&amp; D_{\mathrm{KL} }\left( P_{XY} || P_X P_Y \right) \\<br> &amp;=&amp; \sum_{x,y \in A} P_{XY}(x,y) \log{\frac{P_{XY}(x, y)}{P_X(x) P_Y(y)}} \\<br> &amp;=&amp; \sum_{x,y \in A} \frac{P_{XY}(x,y)}{P_X(x)} P_X(x) \log{\frac{P_{XY}(x, y)}{P_X(x) P_Y(y)}}\\<br> &amp;=&amp; \sum_{x \in A} P_X(x) \sum_{y \in A} P_{Y|X}(x, y) \log{\frac{P_{Y|X}(x,y)}{P_Y(y)}} \\<br> &amp;=&amp; D_{\mathrm{KL} }\left( P_{Y|X} || P_Y | P_X \right) \\<br> &amp;=&amp; D_{\mathrm{KL} }\left( P_{X|Y} || P_X | P_Y \right) \end{eqnarray} $$</p> <p>$$ \begin{eqnarray} \mathrm{I}(X,Y) &amp;=&amp; D_{\mathrm{KL} }\left( P_{XY} || P_X P_Y \right) \\<br> &amp;=&amp; \sum_{x,y \in A} P_{XY}(x,y) \log{\frac{P_{XY}(x, y)}{P_X(x) P_Y(y)}} \\<br> &amp;=&amp; \sum_{x,y \in A} P_{XY}(x,y) \log{P_{XY}(x, y)} - \sum_{x,y \in A} P_{XY}(x,y) \log{P_{X}(x)} - \sum_{x,y \in A} P_{XY}(x,y) \log{P_{Y}(y)} \\<br> &amp;=&amp; - H(X,Y) + H(X) + H(Y) \\<br> &amp;=&amp; H(Y) - H(Y|X) \\<br> &amp;=&amp; H(X) - H(X|Y) \end{eqnarray} $$</p> <p>As mentioned earlier Entropy is a measure of uninformativeness of a distribution. Hence the equations above qualitatively translate to the statement: Mutual Information is the uninformativeness in \( Y \) minus uninformativeness in \( Y \) given \( X \) and symmetrically other way round.</p> <h1 id="relative-entropy-in-machine-learning">Relative Entropy in Machine Learning</h1> <p>In Machine Learning, we are often trying to find the best set of parameters (through optimization) for the probability distribution which best describes the observed data. Since, relative entropy behaves like a distance metric (again, it is not a true metric but is <em>like</em> a metric) in the space of probability distributions, it is a good candidate to be used as the loss function for this optimization. However, in order to use relative entropy as the loss function we require two distributions. They can be:</p> <ol> <li> <p>prior and posterior distributions, in that case we maximize the relative entropy,</p> </li> <li> <p>or one of them can be a non-parametric (empirical) distribution obtained from observed data with the other being the parametric distribution whose parameters we are trying to find</p> </li> </ol> <p>The second case often arises in classification task and we will have a look at this in detail.</p> <h2 id="multiclass-classification">Multiclass classification</h2> <p>A typical multiclass classification problem can be described as follows:</p> <ol> <li> <p>The output \(y\) can take values from a set of \(k\) categories, say \( \{1, 2, … , k\}\)</p> </li> <li> <p>The input is a vector \(x \in R^{r}\)</p> </li> <li> <p>We have \(N\) samples in our dataset: \(\{(y^1, \mathbf{x}^1), (y^2, \mathbf{x}^2), …, (y^N, \mathbf{x}^N) \}\)</p> </li> <li> <p>Given an input \(\mathbf{x}\) our model \(P(Y| X; \theta)\) outputs the probability distribution for \(y\) over the categories \( \{1, 2, … , k\}\). We pick the category with the highest probability as the predicted output. Here, \(P\) is the parametric probability distribution for the output \(Y\) given the input \(X\). Now, this function \(P\) can be modeled in any form: logistic regression, neutral network, etc. Whatever, the model be, it will have a set of parameters \(\mathbf{\theta}\) which we are interested in finding through optimization.</p> </li> </ol> <p>In order to use relative entropy as the loss function, we first construct a categorical distribution \(G_{Y|X=\mathbf{x}}\) for every sample \( (y^i, \mathbf{x}^i) \).</p> <p>$$ G_{Y|X=\mathbf{x}^i}(y) = \left[y \equiv y^i \right] = \begin{cases} 1 &amp; \text{,if } y = y^i \\\\ 0 &amp; \text{otherwise}\end{cases} $$</p> <p>It has to be noted that \( G\) is non-parametric and hence a constant w.r.t \( \theta \) consequently \( H(G) \) is also constant w.r.t \( \theta \). Hence \( \underset{\theta}\arg \min D_{\mathrm{KL}}\left(G_{Y|X=\mathbf{x}^i}(y)||P_{Y|X=\mathbf{x}^i}(y;\theta) \right) \) becomes \( \underset{\theta}\arg \min H\left(G_{Y|X=\mathbf{x}^i}(y), P_{Y|X=\mathbf{x}^i}(y;\theta)\right)\). So the loss function can be taken as the following:</p> <p>$$ \begin{eqnarray} L(\theta) &amp;=&amp; \sum_{i=1}^{N} H(G_{Y|X=\mathbf{x}^i}(y), P_{Y|X=\mathbf{x}^i}(y;\theta)) \\<br> &amp;=&amp; \sum_{i=1}^{N} \sum_{j=1}^{k} G_{Y|X=\mathbf{x}^i}(j) \left[-\log{\left(P_{Y|X=\mathbf{x}^i}(j;\theta)\right)} \right] \\<br> &amp;=&amp; \sum_{i=1}^{N} \sum_{j=1}^{k} \left[j \equiv y^i \right] \left[-\log{\left(P_{Y|X=\mathbf{x}^i}(j;\theta)\right)} \right] \end{eqnarray} $$</p> <p>You can take the two class classification problem and logistic regression model for \( P \) and substitute these into the loss function mentioned above. It will simplify to give the familiar negative log-likelyhood loss function of logistic regression.</p> <h1 id="references">References</h1> <p><a name="1">[1]</a> Kullback, S. (1959), Information Theory and Statistics, John Wiley &amp; Sons. Republished by Dover Publications in 1968; reprinted in 1978: ISBN 0-8446-5625-9.</p> <p><a name="2">[2]</a> <a href="http://videolectures.net/nips09_verdu_re/" rel="external nofollow noopener" target="_blank"> Lecture on Relative Entropy by Sergio Verdu in NIPS 2009 </a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/signal-propagation-on-slurm/">Signal Propagation On Slurm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/vim_setup/">VIMing on Mac</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Dhruvesh Patel. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: August 06, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-115661902-2"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-115661902-2");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>