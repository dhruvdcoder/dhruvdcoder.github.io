---
layout: about
title: about
permalink: /
#description: <a href="#current_affiliations">Current </a> and <a href="#past_affiliations">past</a> affiliations.


profile:
  align: right
  image: dp.jpg
  address: >
     <p>dhruveshpate@umass.edu</p>
  image_circular: false # crops the image to make it circular


news: true  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
collaborators: true
timeline: true
social: true  # includes social icons at the bottom of the page
---


I am currently a fourth-year Computer Science PhD student at [UMass Amherst](https://www.umass.edu/) working with [Prof. Andrew McCallum](https://people.cs.umass.edu/~mccallum) alongside some amazing colleagues at the [Information Extraction and Synthesis Laboratory](https://iesl.cs.umass.edu/).
I completed my undergraduate at [IIT Madras](https://www.iitm.ac.in), where I worked on Robotics research, mentored by [Prof. Bandyopadhyay](https://ed.iitm.ac.in/~sandipan).

Outside of my academic pursuits, I've been fortunate to have worked with some amazing collaborators from the industry. I have worked as a research scientist inter at [Meta Reality Labs](https://ai.meta.com/) and [Abridge AI](https://www.abridge.com/ai/publications).
Before beginning my master's program at UMass, I worked for two years as a software engineer at [MathWorks](https://www.mathworks.com/).
I also dedicated a year to collaborating with [Prof. Partha Talukdar](http://talukdar.net) on solving various NLP problems in the industry.

<br><br>
CV available at the bottom of this page.

## research

Autoregressive models dominate the scene for generative modeling of non-ordinal discrete data, like text, mostly due to the scalability of pre-training.
However, as generative models they have many limitations: limited conditioning and control at inference time, inefficient use of inference time computation by tying the sequence length to the computation, and inability to support non-sequential forms of interaction like edits or deletions.
I'm interested in scaling non-autoregessive models like discrete diffusion and flows for text generation either by adapting pre-trained AR models through continued training or by making non-AR pre-training more efficient.

Prior to this, I have worked on non-Euclidean representation learning, energy-based models for discrete data, and compositional generalization in-context learning.


