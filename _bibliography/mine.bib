---
---

@InProceedings{10.1007/978-3-319-44156-6_12,
author={Dhruvesh Patel and Rohit Kalla and Tetik Halil  and  Kiper  G{\"o}khan  and Sandipan Bandyopadhyay},
editor="Wenger, Philippe
and Flores, Paulo",
title="Computing the Safe Working Zone of a 3-RRS Parallel Manipulator",
booktitle="New Trends in Mechanism and Machine Science",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="113--120",
url="https://link.springer.com/chapter/10.1007/978-3-319-44156-6_12",
abstract="Determination of the safe working zone (SWZ) of a parallel manipulator is a one-time computational task with several permanent benefits. As this subspace of the workspace of the manipulator is free of both the loss- and gain-type singularities, link interference, as well as physical joint limits, the manipulator can move freely in this space. Moreover, if the natural choice of a convex-shaped SWZ is adhered to, then point-to-point path planning inside the SWZ always has a trivial solution, namely, a segment joining the two points, which is guaranteed to be inside the workspace. In this paper, the SWZ of the 3-RRS existing in the {\.{I}}zmir Institute of Technology has been computed. Starting with the geometry of the manipulator, the loop-closure constraint equations have been derived. The singularity conditions are obtained based on the singularity of certain Jacobian matrices associated with the constraint functions. The interference between the links are detected by first encapsulating the links in rectangular parallelepipeds, which are then discretized into triangles, and subjected to collision tests between the relevant pairs of triangles. Using these theoretical developments, the SWZ is computed. The numerical results are depicted graphically.",
isbn="978-3-319-44156-6"
}

@inproceedings{mishra2020looking,
      title={Looking Beyond Sentence-Level Natural Language Inference for Downstream Tasks},
      author={*Anshuman Mishra and *Dhruvesh Patel and *Aparna Vijayakumar and Xiang Li and Pavan Kapanipathi and Kartik Talamadupula},
      booktitle={ArXiv},
      month="December",
      year={2020},
      eprint={2009.09099},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      code={https://github.com/nli-for-qa},
      url={https://arxiv.org/abs/2009.09099},
      abstract={In recent years, the Natural Language Inference (NLI) task has garnered significant attention, with new datasets and models achieving near human-level performance on it. However, the full promise of NLI -- particularly that it learns knowledge that should be generalizable to other downstream NLP tasks -- has not been realized. In this paper, we study this unfulfilled promise from the lens of two downstream tasks: question answering (QA), and text summarization. We conjecture that a key difference between the NLI datasets and these downstream tasks concerns the length of the premise; and that creating new long premise NLI datasets out of existing QA datasets is a promising avenue for training a truly generalizable NLI model. We validate our conjecture by showing competitive results on the task of QA and obtaining the best reported results on the task of Checking Factual Correctness of Summaries.}

}

@inproceedings{mishra2020reading,
      title={Reading Comprehension as Natural Language Inference: A Semantic Analysis},
      author={*Anshuman Mishra and *Dhruvesh Patel and *Aparna Vijayakumar and Xiang Li and Pavan Kapanipathi and Kartik Talamadupula},
      booktitle={StarSem 2020 Workshop at COLING },
      year={2020},
      eprint={2010.01713},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      code={https://github.com/nli-for-qa},
      url={https://arxiv.org/abs/2010.01713},
      abstract={In the recent past, Natural language Inference (NLI) has gained significant attention, particularly given its promise for downstream NLP tasks. However, its true impact is limited and has not been well studied. Therefore, in this paper, we explore the utility of NLI for one of the most prominent downstream tasks, viz. Question Answering (QA). We transform the one of the largest available MRC dataset (RACE) to an NLI form, and compare the performances of a state-of-the-art model (RoBERTa) on both these forms. We propose new characterizations of questions, and evaluate the performance of QA and NLI models on these categories. We highlight clear categories for which the model is able to perform better when the data is presented in a coherent entailment form, and a structured question-answer concatenation form, respectively.}
}

@inproceedings{mishra-etal-2021-looking,
    title = "Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization",
    author = "Mishra, Anshuman  and
      Patel, Dhruvesh  and
      Vijayakumar, Aparna  and
      Li, Xiang Lorraine  and
      Kapanipathi, Pavan  and
      Talamadupula, Kartik",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.104",
    code = "https://github.com/nli-for-qa",
    doi = "10.18653/v1/2021.naacl-main.104",
    pages = "1322--1336",
    abstract = "Natural Language Inference (NLI) has garnered significant attention in recent years; however, the promise of applying NLI breakthroughs to other downstream NLP tasks has remained unfulfilled. In this work, we use the multiple-choice reading comprehension (MCRC) and checking factual correctness of textual summarization (CFCS) tasks to investigate potential reasons for this. Our findings show that: (1) the relatively shorter length of premises in traditional NLI datasets is the primary challenge prohibiting usage in downstream applications (which do better with longer contexts); (2) this challenge can be addressed by automatically converting resource-rich reading comprehension datasets into longer-premise NLI datasets; and (3) models trained on the converted, longer-premise datasets outperform those trained using short-premise traditional NLI datasets on downstream tasks primarily due to the difference in premise lengths.",
    selected=true,
    abbr="NAACL",
    bibtex_show=true,
}

@inproceedings{patel-etal-2020-weakly,
    title = "Weakly Supervised Medication Regimen Extraction from Medical Conversations",
    author = "Patel, Dhruvesh  and
      Konam, Sandeep  and
      Prabhakar, Sai",
    booktitle = "Proceedings of the 3rd Clinical Natural Language Processing Workshop",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.clinicalnlp-1.20",
    doi = "10.18653/v1/2020.clinicalnlp-1.20",
    pages = "178--193",
    abstract = "Automated Medication Regimen (MR) extraction from medical conversations can not only improve recall and help patients follow through with their care plan, but also reduce the documentation burden for doctors. In this paper, we focus on extracting spans for frequency, route and change, corresponding to medications discussed in the conversation. We first describe a unique dataset of annotated doctor-patient conversations and then present a weakly supervised model architecture that can perform span extraction using noisy classification data. The model utilizes an attention bottleneck inside a classification model to perform the extraction. We experiment with several variants of attention scoring and projection functions and propose a novel transformer-based attention scoring function (TAScore). The proposed combination of TAScore and Fusedmax projection achieves a 10 point increase in Longest Common Substring F1 compared to the baseline of additive scoring plus softmax projection.",
    selected=true,
    abbr="ACL",
    bibtex_show=true,
}

@inproceedings{
dhruveshbox2020,
title={Representing Joint Hierarchies with Box Embeddings},
author={*Dhruvesh Patel  and *Shib Sankar Dasgupta and Michael Boratko and Xiang Li and Luke Vilnis and Andrew McCallum},
booktitle={Automated Knowledge Base Construction (AKBC)},
year={2020},
url={https://openreview.net/forum?id=J246NSqR_l},
abstract="Learning representations for hierarchical and multi-relational knowledge has emerged as an active area of research. Box Embeddings  [Vilnis et al., 2018, Li et al., 2019] represent concepts with hyperrectangles in n-dimensional space and are shown to be capable of modeling tree-like structures efficiently by training on a large subset of the transitive closure of the WordNet hypernym graph. In this work, we evaluate the capability of box embeddings to learn the transitive closure of a tree-like hierarchical relation graph with far fewer edges from the transitive closure. Box embeddings are not restricted to tree-like structures, however, and we demonstrate this by modeling the WordNet meronym graph, where nodes may have multiple parents. We further propose a method for modeling multiple relations jointly in a single embedding space using box embeddings. In all cases, our proposed method outperforms or is at par with all other embedding methods.",
slides={akbc2020-slides.pdf},
video={https://youtu.be/yqP8wjMocAs},
code={https://github.com/iesl/Boxes_for_Joint_hierarchy_AKBC_2020},
selected=true,
bibtex_show=true,
}
